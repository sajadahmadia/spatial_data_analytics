# -*- coding: utf-8 -*-
"""Group5_Case_Study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LYwHPtMd20CLu4a1qIYaHHpg4DNNW7j0

# Active Transport in Amsterdam

Measuring Bikeablity in Amsterdam using:


*   Infrastrucutre quality
*   Bike parkings
*   Greenness
"""

# installing the main libraries
!pip install googletrans==4.0.0-rc1
!pip3 install geopandas
!apt-get install gdal-bin
!pip install osmnx

# importing the libraries
import geopandas as gpd
import folium
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from googletrans import Translator
import folium
import osmnx as ox
from branca.colormap import linear

# using amsterdam.nl Geojson maps
pc4_data = 'https://maps.amsterdam.nl/open_geodata/geojson_lnglat.php?KAARTLAAG=PC4_BUURTEN&THEMA=postcode'
greenness_data = 'https://maps.amsterdam.nl/open_geodata/geojson_lnglat.php?KAARTLAAG=PARKPLANTSOENGROEN&THEMA=stadsparken'
network_data = 'https://maps.amsterdam.nl/open_geodata/geojson_lnglat.php?KAARTLAAG=FIETSNETTEN&THEMA=fietsnetten'
buildings_data = ""

# Load the green areas data
green = gpd.read_file(greenness_data).to_crs(epsg=28992)

# Load the postal code areas data
pc4 = gpd.read_file(pc4_data).to_crs(epsg=28992)

# Load the biking network data
bikenet = gpd.read_file(network_data).to_crs(epsg=28992)

# looking at the first 5 rows
pc4.head()

bikenet.head()

"""## Defining the translation functions
To translate the dataframes' data from Dutch to English
"""

def column_trans(dataframe): #provide the dataframe
  translator = Translator()
  translated_columns = {col: translator.translate(col, src='nl', dest='en').text.lower() for col in dataframe.columns}
  dataframe.rename(columns=translated_columns, inplace = True)
  return dataframe

def column_values_trans(dataframe, columns_list = []): #provide the list of columns that you want to be translated
  translator = Translator()
  for col in columns_list:
    unique_values = list(dataframe[col].unique())
    translated_values = {val: translator.translate(val, src='nl', dest='en').text.lower() for val in unique_values}
    dataframe[col] = dataframe[col].map(translated_values)
  return dataframe

"""Translating the bikenet dataset"""

pc4.columns = [x.lower() for x in pc4.columns]
#postal_code_gdf = column_values_trans(postal_code_gdf, columns_list = [])
pc4.head()

bikenet.head()

bikenet = column_trans(bikenet)
bikenet.head()

# translating the "label","kind","hardening" columns content into English
bikenet = column_values_trans(bikenet, columns_list = ["label","kind","hardening"])

"""## Measuring the infrastructure quality
At this step, we first map each bike lane type into a value, called qulaity score of that bike lane type, based on the indices provided by Schmid et al.(2021)
"""

# finding the unique values in the kind column
bikenet.kind.unique()

# assinging weights to each network infrastructre type
bikenet_qulaity = {'cycle street': 7,
                   'bike path' : 10,
                   'cycle path (moped not allowed)' : 8,
                   'bicycle lane' : 6,
                   'shared space' : 3,
                   'bicycle crossing' : 3,
                   'link' : 3,
                   'moped/cycle path' : 9,
                   'cycle path (without obligation)' : 5,
                   'bike on a roadway' : 3
                   }

bikenet['quality_grade'] = bikenet['kind'].map(bikenet_qulaity)

bikenet.head()

"""Finding the intersection of bikenet and pc4 areas:
we need to first apply spatial join and then

then, we apply map overlay and intersection operation to find the intersection of the bike network in each pc4 polygon(line in polygon operation):
"""

intersected_segments = gpd.overlay(bikenet, pc4, how='intersection')
intersected_segments['segment_length_m'] = intersected_segments.geometry.length
intersected_segments.head()

"""At this part, we find the weighted average of the segment_length_m column per quality grade:"""

# measuring the sum over rows
intersected_segments['sums'] = intersected_segments.apply(lambda row: row["segment_length_m"]*row["quality_grade"], axis = 1)

# measuing the weighted average of the network quality grade per PC4
pc4_quality = intersected_segments.groupby("Postcode4")['sums'].sum() / intersected_segments.groupby("Postcode4")['segment_length_m'].sum()

# counting the number of pc4 areas:
len(pc4_quality)

"""As we can see, 2 neighborhoods(out of 85) don't have a bike network. As a result, aren't mentioned in our data."""

# showing the results:
pc4_quality

# exporting the results to geojson and csv files:
intersected_segments.to_file("bikenet_infra_quality.json", driver="GeoJSON")
pc4_quality.to_csv("final.csv")

"""### Defining plot functions"""

#Function to plot geodataframes using folium
def plot_geo(dataframe):
    m = folium.Map(location=[52.3676, 4.9041], zoom_start=13)

    # Add Thunderforest OpenCycleMap tiles using your API key
    api_key = 'eacf9f1005b547cf86659bc68d653b7f'
    tile_url = f'https://tile.thunderforest.com/cycle/{{z}}/{{x}}/{{y}}.png?apikey={api_key}'
    folium.TileLayer(tile_url, attr='Thunderforest OpenCycleMap', name='OpenCycleMap').add_to(m)

    # Add the buffered bike lanes to the map
    folium.GeoJson(dataframe, name="Buffered Bike Lanes", style_function=lambda x: {'color': 'blue', 'opacity': 0.5}).add_to(m)

    # Add a layer control panel to the map
    folium.LayerControl().add_to(m)

    # Display the map
    return m

#Function to plot geodataframes with ranking based on features
def plot_geomap(dataframe, feature_column):
    # Create a colormap scaled from 0 to 10
    colormap = linear.RdYlGn_11.to_step(5).scale(0, 10)

    # Create a map
    m = folium.Map(location=[52.3676, 4.9041], zoom_start=13)

    # Add Thunderforest OpenCycleMap tiles using your API key
    api_key = 'eacf9f1005b547cf86659bc68d653b7f'
    tile_url = f'https://tile.thunderforest.com/cycle/{{z}}/{{x}}/{{y}}.png?apikey={api_key}'
    folium.TileLayer(tile_url, attr='Thunderforest OpenCycleMap', name='OpenCycleMap').add_to(m)

    folium.GeoJson(dataframe,
                   name="Bike Parking Quality Per PC4",
                   style_function=lambda x: {
                       'color': colormap(x['properties'][feature_column]),
                       'opacity': 1
                   }).add_to(m)

    # Add a colormap legend
    colormap.caption = f'{feature_column.capitalize()} (0-10)'
    colormap.add_to(m)

    # Add a layer control panel to the map
    folium.LayerControl().add_to(m)

    return m

"""# Bike Parking

We use the OpenStreetMap (OSM) data retrieval library osmnx to obtain geometries of bicycle parking facilities within Amsterdam. These geometries are then organized into a GeoDataFrame for further spatial analysis. The resulting bike_amenities_gdf GeoDataFrame contains information about the location, type, and geometry of bicycle parking facilities in Amsterdam.
"""

place_name = "Amsterdam, Netherlands"

bike_parking = ox.geometries_from_place(place_name, tags={'amenity': 'bicycle_parking'})

# Convert to a GeoDataFrame
bike_amenities_gdf = gpd.GeoDataFrame(bike_parking, geometry='geometry')

#Dutch national projection system (EPSG:28992)
bike_amenities_gdf = bike_amenities_gdf.to_crs(epsg=28992)

bike_amenities_gdf

#drop the second row to orgainze the geodataframe
bike_amenities_gdf.reset_index(drop=True, inplace=True)
bike_amenities_gdf= bike_amenities_gdf.drop(1)
bike_amenities_gdf

#drop null values from "bicycle_parking" column
bike_amenities_gdf = bike_amenities_gdf.dropna(subset=['bicycle_parking'])
bike_amenities_gdf

#plot the geodataframe
plot_geo(bike_amenities_gdf)

#check tyoes of bicycle_parking in the geodataframe
bike_amenities_gdf.bicycle_parking.unique()

unique_counts = bike_amenities_gdf['bicycle_parking'].value_counts()

print(unique_counts)

""" Now, we are going to clean and reorganize a dataset containing information about bicycle parking amenities. we will srart by categorizing various types of bicycle racks into four main categories: regular racks, roofed racks, bike lockers, and no bike racks. This categorization is done by creating replacement dictionaries for each category and using them to replace the values in the 'bicycle_parking' column. After the replacement, we remove rows that contain other values from the dataset, leaving only the cleaned data with the four main categories of bicycle parking amenities."""

regular_rack = ['stands', 'rack', 'wall_loops', 'safe_gutter', 'safe_loops', 'ground_slots', 'standss', 'double_rack', 'two-tier', 'racks']  # List all values you want to replace
regular = 'regular_rack'
roofed_rack = ['shed', 'building', 'floor', 'building2', 'boat', 'staple', 'high_capacity']
roofed ='roofed_rack'
bike_locker = ['handlebar_holder', 'lockers']
locker = 'bike_locker'
no_bike_rack = ['anchors']
no_rack = ['no_bike_rack']
# Use a dictionary comprehension to create a dictionary for replacement
replacement_dict_1 = {old_value: regular for old_value in regular_rack}
replacement_dict_2 = {old_value: roofed for old_value in roofed_rack}
replacement_dict_3 = {old_value: locker for old_value in bike_locker}
replacement_dict_4 = {old_value: no_rack for old_value in no_bike_rack}

# Replace the values
bike_amenities_gdf['bicycle_parking'] = bike_amenities_gdf['bicycle_parking'].replace(replacement_dict_1)
bike_amenities_gdf['bicycle_parking'] = bike_amenities_gdf['bicycle_parking'].replace(replacement_dict_2)
bike_amenities_gdf['bicycle_parking'] = bike_amenities_gdf['bicycle_parking'].replace(replacement_dict_3)
bike_amenities_gdf['bicycle_parking'] = bike_amenities_gdf['bicycle_parking'].replace(replacement_dict_4)
values_to_drop = ['regular_rack', 'roofed_rack' , 'bike_locker', 'no_bike_rack']
mask = bike_amenities_gdf['bicycle_parking'].isin(values_to_drop)
bike_amenities_gdf=bike_amenities_gdf[mask].copy()

bike_amenities_gdf

unique_counts = bike_amenities_gdf['bicycle_parking'].value_counts()
print(unique_counts)

"""We are going to  assign quality grades to different bicycle parking facilities based on their types by creatung a dictionary that maps each type of bicycle parking facility to a quality grade or wieght. This process allows for the categorization and assessment of the quality of bicycle parking amenities within the dataset."""

bike_facilities_qulaity = {'regular_rack': 6,
                   'roofed_rack' : 8,
                   'bike_locker' : 10,
                   'no_bike_rack' : 2,
                   }

bike_amenities_gdf['quality_grade'] = bike_amenities_gdf['bicycle_parking'].map(bike_facilities_qulaity)

bike_amenities_gdf.head()

#drop useless columns
columns_to_drop = bike_amenities_gdf.columns[4:59]
bike_amenities_gdf = bike_amenities_gdf.drop(columns=columns_to_drop)
bike_amenities_gdf

#convert the ploygon into points by finding their centroids
bike_amenities_gdf['centroid'] = bike_amenities_gdf.geometry.centroid

bike_parking_points = gpd.GeoDataFrame(bike_amenities_gdf, geometry=bike_amenities_gdf['centroid'])

bike_parking_points = bike_parking_points[['bicycle_parking', 'quality_grade', 'centroid']]

bike_parking_points.rename(columns={'centroid': 'geometry'}, inplace=True)
bike_parking_points

unique_counts = bike_parking_points['bicycle_parking'].value_counts()

print(unique_counts)

#join PC4 areas with the bike parking points
bike_parking_pc4 = gpd.sjoin(pc4, bike_parking_points, how='inner', op='intersects')

bike_parking_pc4

unique_counts = bike_parking_pc4['postcode4'].value_counts()

print(unique_counts)

"""We can see here the variation in the number of bike parking facilities per postcode 4 areas.

Now, we will group the data by 'postcode4,' and calculate two key metrics per postal code area: the 'parking_count,' which counts the number of bicycle parking facilities in each area, and 'avg_quality,' which computes the average quality grade of these facilities.
"""

bike_parking_pc4['quality_grade'] = pd.to_numeric(bike_parking_pc4['quality_grade'], errors='coerce')

# group by 'postcode4'
grouped_bike_parking_pc4 = bike_parking_pc4.groupby('postcode4').agg(
    parking_count=('bicycle_parking', 'size'),  # Count the number of bicycle parking per pc4
    avg_quality=('quality_grade', 'mean')       # Calculate the average quality grade per pc4
).reset_index()

grouped_bike_parking_pc4

"""We calculate a weighted score for bike parking facilities in different postal code areas (PC4). We define weights for the quality and count of parking facilities, with both weights set to 0.5 arbitrarily.  Then calculate the maximum count of parking facilities in any PC4 area and normalizes the count scores to a scale from 0 to 10. Finally, we compute the weighted score for each PC4 area by combining the normalized count score and the quality score (averaged quality score for each PC4 area).

**Parking Service Quality=(Average Quality×Quality Weight)+(Parking Count×Count Weight)**

Where:

**Average Quality**: the average quality rating from 0 to 10.

**Parking Count:** the count of parking facilities.

**Quality Weight:** the weight you assign to the average quality rating.

**Count Weight:** the weight you assign to the count of parking facilities.
"""

# Define weights
quality_weight = 0.5
count_weight = 0.5


#max_score = grouped_bike_parking_pc4['parking_count'].max()
max_score = grouped_bike_parking_pc4['parking_count'].max()
# Normalize the scores to a 0-10 scale
grouped_bike_parking_pc4['parking_count_score'] =(grouped_bike_parking_pc4['parking_count'] / max_score) * 10
# Calculate the weighted score
grouped_bike_parking_pc4['parking_quality'] = ((grouped_bike_parking_pc4['avg_quality'] * quality_weight) + (grouped_bike_parking_pc4['parking_count_score'] * count_weight))

grouped_bike_parking_pc4

max_score = grouped_bike_parking_pc4['parking_quality'].max()
max_score

#merge the dataframe with PC4 areas geodataframe
bike_parking_quality_pc4 = pc4.merge(grouped_bike_parking_pc4, on='postcode4')
bike_parking_quality_pc4

plot_geomap(bike_parking_quality_pc4, 'parking_quality')

bike_parking_quality_pc4.to_file("bike_parking_quality_pc4.geojson", driver='GeoJSON')

from google.colab import drive
drive.mount('/content/drive')

"""## Greenness"""

# get percentage of green output from geojson
percentage_of_green = gpd.read_file('/content/sample_data/final_greenness.geojson')
percentage_of_green.head()

"""## Final results

At this step, we combine all the metrics with proper weights fo measure a finalized score for each PC4.
"""

# importing the required libraries
from sklearn.preprocessing import MinMaxScaler

# sclaing each measure to [0,1] range
def scaler(dataframe, column):
  scaler = MinMaxScaler()
  name = f"scaled_{column}".format(column)
  dataframe[name] = scaler.fit_transform(dataframe[column].values.reshape(-1,1))
  return dataframe

percentage_of_green = scaler(percentage_of_green, 'percentage_of_green')
percentage_of_green.columns = [x.lower() for x in percentage_of_green.columns]

percentage_of_green.head()

# reading in 3 seprated datasets:
bike_parking = gpd.read_file('/content/sample_data/bike_parking_quality_pc4.geojson')
infra_quality = pd.read_csv('/content/sample_data/final_infra_quality.csv')
infra_quality.columns = ['postcode4','quality']

bike_parking.head()

infra_quality.head()

# applying the sclaer function to datasets
bike_parking = scaler(bike_parking, 'parking_quality' )
infra_quality = scaler(infra_quality, 'quality')

bike_parking.head()

infra_quality.head()

# changing the datatype of postcode4 in the greenness dataset to int to prepare it for joining
percentage_of_green['postcode4'] = percentage_of_green['postcode4'].astype('int')

# merging datasets
scaled_measures = pd.merge(infra_quality[['postcode4','scaled_quality']],bike_parking[['postcode4','scaled_parking_quality']],
          how = 'inner').merge(percentage_of_green[['postcode4','scaled_percentage_of_green']], how = 'inner')

scaled_measures

# defining a function to combine all measures
def scoring(dataframe, indices = [1,2,3], weights = [0.5,0.25, 0.25]):
  result = 0
  for i,j in zip(indices, weights):
    result += dataframe.iloc[:,i] * j
  dataframe['final_score'] = result
  return dataframe

# combining all measures into 1 unified scale
scaled_measures = scoring(scaled_measures)
scaled_measures

# visualing the histogram of the final version
from matplotlib import pyplot as plt
scaled_measures['scaled_percentage_of_green'].plot(kind='hist', bins=20, title='final_score')
plt.gca().spines[['top', 'right',]].set_visible(False)

# saving the rsult to a file
scaled_measures.to_csv("scaled_measures.csv")
